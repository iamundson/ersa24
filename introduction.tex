% PROVERS
Formal methods provides a mathematically rigorous means of verification that one would expect for the development of high-assurance systems such as those in the aerospace and defense industries. Certification guidance has even been published on how formal methods can be used to satisfy airworthiness objectives for airborne software in commercial aircraft~\cite{DO-333}.  However, despite the effectiveness of these powerful proof techniques, their adoption into traditional development processes has been slow and uneven.  Reasons for slow uptake include scalability limitations of the underlying algorithms, poorly designed user interfaces and other tool usability factors, and the need for formal training to properly use them~\cite{davis-fmics13}.

The DARPA Pipelined Reasoning of Verifiers Enabling Robust Systems (PROVERS) program was recently launched with the goal of producing scalable and usable formal methods tools that can be integrated into traditional aerospace and defense development processes.  Specifically, a key outcome of the program is that product engineers with minimal formal methods background will be able to benefit from these powerful technologies, further driving their adoption while simultaneously improving product dependability.

% INSPECTA
To address these challenges, our team is developing the Industrial-Scale Proof Engineering for Critical Trustworthy Applications (INSPECTA) framework\footnote{https://loonwerks.com/projects/inspecta.html}.  INSPECTA consists of \textit{ProofOps} and \textit{BuildOps} tools and methods that integrate with current aerospace DevOps pipelines and achieve provably correct design and implementation at each level of the system hierarchy.  In order to address the key objectives of PROVERS, we pay particular attention to addressing scalability and explainability concerns with respect to the proof tools in our framework.

% Compositional Reasoning
Within the \textit{ProofOps} workflow, INSPECTA uses the Assume-Guarantee Reasoning Environment (AGREE)~\cite{compositional-analysis-agree}, a formal compositional reasoning tool for Architecture Analysis and Design Language (AADL)~\cite{feiler-aadl} models.
Compositional reasoning partitions the formal analysis of a complex system architecture into verification tasks corresponding to the architecture's decomposition.  By partitioning the verification effort into proofs about each subsystem within the architecture, the analysis will scale to handle large system designs.

Although AGREE does not suffer from some of the scalability issues inherent in other formal methods frameworks due to the compositional nature of the analysis, generated counterexamples can still be difficult to understand, especially for formal methods novices when the counterexamples contain several steps, each consisting of multiple variables.  This problem is not unique to AGREE, but is common to most model checkers in use today~\cite{cex-explanation}.  
Recently, however, a novel approach to producing explainable counterexamples has emerged in the form of generative AI.

Research on applying generative AI to formal reasoning has already gained significant attention. For instance, OpenAI researchers conducted pioneering work in 2020, leveraging large language models (LLMs) for mechanical theorem proving~\cite{polu2020generative}. This resulted in the development of GPT-f, a proof assistant for Metamath, which achieved a 56\% success rate and proved 200 theorems~\cite{megill2019metamath}. Other studies have explored LLMs for proof generation and repair. First et al. achieved a 50\% success rate in proof repair for Isabelle/HOL~\cite{first2023baldur}, using Minerva~\cite{lewkowycz2022solving}, a model based on Googleâ€™s PaLM~\cite{chowdhery2022palm}. Research has also examined GPT-3.5 and GPT-4 for Coq theorem proving~\cite{zhang2023getting}, primarily focusing on diagnosing failed proofs. LLMs have further been applied to discover program invariants~\cite{pei2023can, wu2023lemur} and support automated reasoning, as seen in the Clover project by Stanford and VMware, which emphasizes verifiable code generation~\cite{sun2024clover}.

In previous work~\cite{CoqDog, CoqDogHCSS24}, Tahat et al. developed a copilot for large-scale proof repair using multi-shot conversational learning. The approach achieved a 97\% success rate across 58 theorems from a repository containing 20,000 lines of Coq code from the Copland proofbase. Additionally, they introduced an evaluation framework to assess the convergence of dialogues toward predefined proof sets.
%
%While this paper builds on the success of~\cite{CoqDog}, it represents a paradigm shift to model repair in the context of model-based systems engineering (MBSE) for AADL models. %It integrates human requirements inputs and context retrieval-augmented generation methods to address and resolve counterexamples detected by the AGREE model checker. 
%To the best of our knowledge, this is the first application of applying generative AI for producing explainable counterexamples and model repair for compositional reasoning in architecture models.

In this paper, we present our current work on using generative AI to provide clear and concise explanations of counterexamples generated by AGREE.  
%
Although using generative AI for explainable formal verification has been explored in other works (e.g.,~\cite{martins-ersa23}), to the best of our knowledge, this is the first application of applying generative AI for producing explainable counterexamples from compositional reasoning over architecture models.
%
Our initial results indicate this approach is well-suited for providing clear explanations of root cause, as well as suggestions for addressing the contract violations.